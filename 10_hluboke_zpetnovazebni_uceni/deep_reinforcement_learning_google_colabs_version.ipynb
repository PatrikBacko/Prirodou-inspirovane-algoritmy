{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hluboké zpětnovazební učení\n",
    "\n",
    "Dneska se podíváme na hluboké zpětnovazební učení, které kombinuje dva námi už známé přístupy -- zpětnovazební učení a hluboké neuronové sítě. Tato technika umožňuje vytvářet mocné agenty, kteří jsou schopni se trénovat sami z hraní her. Tím se učí, jaké tahy jsou dobré a jak hru vyhrát. Takto se agenti naučili některé hry jako dáma a go hrát na úrovni lidských hráčů, a dokonce je i porazit.\n",
    "\n",
    "## Zpětnovazební učení\n",
    "\n",
    "Připomeňme si nejprve zpětnovazební učení, které jsme dělali na druhém cvičení. Jeho cílem je naučit našeho agenta, jak se má co nejlépe chovat v nějakém daném prostředí. Necháme ho vykonávat akce, díky kterým se bude přesouvat z jednoho stavu do druhého a od prostředí za to bude dostávat odměnu, kterou se bude snažit maximalizovat. Zpočátku bude provádět náhodné akce a spíše prozkoumávat prostředí, ale později se bude více snažit najít optimální řešení, čili maximalizovat svůj užitek za své chování. Takto se tedy agent naučí nějakou strategii, jak se co nejlépe v daném prostředí chovat. Nezapomeňme, že jednotlivé stavy, mezi kterými se agent přesouvá, nejsou nezávislé a dají se popsat Bellmannovými rovnicemi.\n",
    "\n",
    "## Q-učení\n",
    "\n",
    "Q-učení je konkrétní druh zpětnovazebního učení, kde se agent učí odhadovat odměnu pro každý stav a v něm vykonanou akci. Odměnu dostane agent za provedení té akce v tom daném stavu. V klasickém Q-učení jsou tyto hodnoty uložené v matici Q, která má jako řádky stavy a jako sloupce akce. Agent si po provedení akce v daném stavu postupně updatuje hodnoty na odpovídajících souřadnicích v matici, aby pro příště věděl, jak výhodné je tu akci v tom stavu udělat. Problém je v prostředí s hodně stavy a akcemi, protože pak budeme mít obrovskou matici. To se dá vyřešit pomocí hlubokého Q-učení, kde místo matice Q, budeme používat neuronovou síť.\n",
    "\n",
    "\n",
    "## Hluboké Q-učení\n",
    "\n",
    "Dnes se tedy podíváme na agenta s hlubokým Q-učením, které kombinuje klasické Q-učení a neuronové sítě. Místo matice Q agentovi definujeme model neuronové sítě, který bude předpovídat hodnoty funkce Q. Síť bude na vstupu dostávat stav a jako výstup bude vracet pravděpodobnosti toho, jak dobré je danou akci udělat. Trénování sítě bude potom probíhat tak, že se porovná aktuální odměna od prostředí s hodnotou spočtenou z Q a minimalizuje se MSE jejich rozdílu. Příklad je inspirován příkladem [odsud](https://keon.io/deep-q-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.nn import relu\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementace třídy agenta je celkem přímočará. Definujeme si síť s daným počtem vstupů a výstupů a nastavíme jí všechny parametry potřebné pro trénování Q-učení. Funkce ```build_model``` vytvoří model neuronové sítě se dvěma skrytými vrstvami,  funkce ```action``` vrací akci. Pokud trénujeme, síť používá epsilon-greedy strategii, pokud jen vyhodnocujeme, vrací nejlepší akci. Funkce ```train``` trénuje samotnou neuronovou síť a updatuje váhy.\n",
    "\n",
    "Protože by ale update vah po každém kroku byl hrozně pomalý a navíc bychom ho dělali jen pro ten jeden poslední krok, uděláme to chytřeji a budeme trénovat síť pomocí tzv. bufferu. Do bufferu si pomocí funkce ```memorize``` budeme ukládat minulý stav, akci, odměnu, nový stav a příznak done, zda se jedná o cílový stav. Po každém kroku si z bufferu vybereme náhodnou trénovací množinu několika minibatchů, na jejichž základě upravíme váhy sítě tak, že spočítáme odměnu pro danou akci a druhou mocninou rozdílu odměny od prostředí a odměny od sítě spočítáme chybu, která se pak backpropaguje zpátky. Toto za nás udělá metoda ```fit```, jen jí musíme předat stavy a požadované výstupy.\n",
    "\n",
    "Protože ale samotné trénování modelu trvá dlouho, napíšeme si ještě dvě funkce pro ukládání a načítání natrénovaného modelu, abychom nemuseli model přetrénovávat a mohli si ho rovnou nahrát s dříve předtrénovanými váhami. Funkce ```save_model``` pouze serializuje model a natrénované váhy do formátu json a uloží je. Funkce ```load_model``` nám naopak načte a nastaví agentovi uložený model a váhy rovnou bez trénovaní."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearningAgent:\n",
    "    def __init__(self, num_inputs, num_outputs, batch_size = 8, num_batches = 16):\n",
    "        self.num_inputs = num_inputs # state size\n",
    "        self.num_outputs = num_outputs # action size\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        \n",
    "        self.epsilon = 1.0 \n",
    "        self.epsilon_decay = 0.995      \n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.build_model()\n",
    "        self.epsilon_min = 0.01\n",
    "                 \n",
    "    def build_model(self):\n",
    "        self.model = Sequential([])\n",
    "        self.model.add(Dense(24, activation=relu, input_dim=self.num_inputs, name='dense_11'))\n",
    "        self.model.add(Dense(24, activation=relu))\n",
    "        self.model.add(Dense(self.num_outputs, activation='linear'))\n",
    "        \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.model.compile(optimizer=opt, loss='mse')\n",
    "    \n",
    "    def action(self, state, train=False):\n",
    "        if train and np.random.uniform() < self.epsilon:\n",
    "            return np.random.randint(self.num_outputs)\n",
    "        else: \n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def train(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            \n",
    "            # samplovani minibatche z pameti\n",
    "            batch = random.sample(self.memory, self.batch_size)\n",
    "            states = np.array([s for (s, _, _, _, _) in batch])\n",
    "            \n",
    "            next_states = np.array([ns for (_, _, _, ns, _) in batch])\n",
    "            states = states.reshape((-1, self.num_inputs))\n",
    "            next_states = next_states.reshape((-1, self.num_inputs))\n",
    "            \n",
    "            # predikce odmen za akce\n",
    "            predicted = self.model.predict(states)\n",
    "            next_predicted = self.model.predict(next_states)\n",
    "                           \n",
    "            # spocteni cilove hodnoty\n",
    "            for i, (state, action, reward, next_state, done) in enumerate(batch):\n",
    "                predicted[i][action] = reward\n",
    "                if not done:\n",
    "                    predicted[i][action] = reward + self.gamma*np.amax(next_predicted[i])\n",
    "\n",
    "            self.model.fit(states, predicted, epochs=1, verbose=0)\n",
    "                           \n",
    "        # snizeni epsilon pro epsilon-greedy strategii\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon*self.epsilon_decay\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def save_model(self):\n",
    "        model_json = self.model.to_json()\n",
    "        \n",
    "        with open('model.json', 'w') as json_file:\n",
    "            json_file.write(model_json)\n",
    "            \n",
    "        self.model.save_weights('model.h5')\n",
    "    \n",
    "    def load_model(self): \n",
    "        json_file = open('model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        \n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        loaded_model.load_weights('model.h5')\n",
    "        \n",
    "        self.model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Když už máme napsaného agenta s hlubokým zpětnovazebním učením, vyzkoušíme si ho na nějakém zajímavém úkolu. Tentokrát si zkusíme prostředí [CartPole-v1](https://gym.openai.com/envs/CartPole-v1/), kde máme tyčku na vozíku, který se pohybuje doprava nebo doleva. Cílem je, aby tyčka nespadla. Odměna +1 je získána za každý časový krok, kdy tyč zůstane vzpřímená. Epizoda končí, když je tyč více než 15 stupňů od svislé osy, nebo když se vozík posune více než 2.4 jednotky od středu.\n",
    "\n",
    "Podíváme se tedy, jak toto prostředí vypadá a pro zajímavost si vypíšeme prostor pozorování a akcí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní už přejdeme k samotnému trénování. Stav je daný pozicí a rychlostí vozíku a úhlem a rychlostí tyčky a akce jsou pohyb doleva nebo doprava. Vytvoříme si tedy agenta se 4 vstupy a 2 akcemi. Dále si vytvoříme prostředí a pomocné pole pro logování odměn. Pak už můžeme pustit samotné trénování na 1000 epochách a na závěr natrénovaný model uložit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DeepQLearningAgent(4, 2)\n",
    "env = gym.make('CartPole-v1')\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for i in range(1001):\n",
    "    observation, _ = env.reset()\n",
    "    observation = np.reshape(observation, newshape=(1, -1))\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    time = 0\n",
    "    while not (done or terminated):\n",
    "        old_state = observation\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "        total_reward += reward\n",
    "        time += 1\n",
    "        reward = reward if not done else 10 # bonus za uplne vyreseni\n",
    "        observation = np.reshape(observation, newshape=(1, -1))\n",
    "        agent.memorize(old_state, action, reward, observation, done)\n",
    "    \n",
    "    agent.train()\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    if i % 100 == 0:\n",
    "        print(i, total_reward)\n",
    "agent.save_model()\n",
    "\n",
    "open_file = open('rewards.txt', 'wb')\n",
    "pickle.dump(rewards, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyzkoušíme, jak dobře náš agent umí problém řešit a zároveň si i zobrazíme animaci. Abychom nemuseli dlouho čekat na natrénování vah, načteme si ty předtrénované."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DeepQLearningAgent(4, 2)\n",
    "agent.load_model()\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "observation, _ = env.reset()\n",
    "observation = np.reshape(observation, newshape=(1, -1))\n",
    "done = False\n",
    "terminated = False\n",
    "total_reward = 0\n",
    "time = 0\n",
    "while not (done or terminated):\n",
    "    env.render()\n",
    "    old_state = observation\n",
    "    action = agent.action(observation, train=False)\n",
    "    observation, reward, done, terminated, _ = env.step(action)\n",
    "    observation = np.reshape(observation, newshape=(1, -1))\n",
    "    total_reward += reward\n",
    "    time += 1     \n",
    "print(total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vykreslíme si výsledný graf odměn. Vidíme, že se celkově výsledky zlepšují, ale občas nám spadnou dolu. To se klidně může stát, protože vykreslujeme skóre z jedné hry a občas nějaká holt nevyjde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file = open('rewards.txt', 'rb')\n",
    "rewards = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(rewards)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Úkol na cvičení\n",
    "\n",
    "Vyberte si z [OpenAI gym](https://gym.openai.com/envs/) některý z problémů a zkuste ho vyřešit pomocí hlubokého Q-učení. Doporučuji si vybrat nějaké s diskrétními akcemi, ale můžete si klidně vybrat i akce spojité, jen je bude potřeba diskretizovat. V případě diskretizace je dobré mít těch akcí spíše méně, aby síť nemusela mít moc výstupů a netrénovala se příliš dlouho. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
